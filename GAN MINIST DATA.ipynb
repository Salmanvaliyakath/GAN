{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5085c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSALMANN\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7625ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More importing common libraries\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c17dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST Data\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Scale the inputs in range of (-1, +1) for better training\n",
    "x_train, x_test = x_train / 255.0 * 2 - 1, x_test / 255.0 * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "308f964c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (60000, 28, 28)\n",
      "x_test.shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print (\"x_train.shape:\" ,x_train.shape)\n",
    "print (\"x_test.shape:\" ,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ad180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data\n",
    "N, H, W = x_train.shape\n",
    "D = H * W\n",
    "x_train = x_train.reshape(-1, D)\n",
    "x_test = x_test.reshape(-1, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f24df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (60000, 784)\n",
      "x_test.shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print (\"x_train.shape:\" ,x_train.shape)\n",
    "print (\"x_test.shape:\" ,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd7eb60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the latent space\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af56ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the generator model\n",
    "def build_generator(latent_dim):\n",
    "    i = Input(shape=(latent_dim,))\n",
    "    x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)\n",
    "    x = BatchNormalization(momentum=0.7)(x)\n",
    "    x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)\n",
    "    x = BatchNormalization(momentum=0.7)(x)\n",
    "    x = Dense(1024, activation=LeakyReLU(alpha=0.2))(x)\n",
    "    x = BatchNormalization(momentum=0.7)(x)\n",
    "    x = Dense(D, activation='tanh')(x)\n",
    "\n",
    "    model = Model(i, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c917c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the discriminator model\n",
    "def build_discriminator(img_size):\n",
    "    i = Input(shape=(img_size,))\n",
    "    x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n",
    "    x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "    model = Model(i, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "986a85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile both models in preparation for training\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(D)\n",
    "discriminator.compile ( loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the combined model\n",
    "generator = build_generator(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "050ec96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               25856     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 784)               803600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "026626a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b9a726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an input to represent noise sample from latent space\n",
    "z = Input(shape=(latent_dim,))\n",
    "\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75949bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 784) dtype=float32 (created by layer 'model_1')>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass noise through generator to get an image\n",
    "img = generator(z)\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2be23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure only the generator is trained\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ae2982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The true output is fake, but we label them real!\n",
    "# Passing the output of Generator to the Discriminator\n",
    "\n",
    "fake_pred = discriminator(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b47ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the combined model object\n",
    "combined_model_gen = Model(z, fake_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "766129bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the combined model\n",
    "combined_model_gen.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "804124fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 30000\n",
    "sample_period = 200 # every `sample_period` steps generate and save some data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86915383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch labels to use when calling train_on_batch\n",
    "ones = np.ones(batch_size)\n",
    "zeros = np.zeros(batch_size)\n",
    "\n",
    "# Store the losses\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "# Create a folder to store generated images\n",
    "if not os.path.exists('gan_images'):\n",
    "    os.makedirs('gan_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79c89c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to generate a grid of random samples from the generator and save them to a file\n",
    "\n",
    "def sample_images(epoch):\n",
    "    rows, cols = 5, 5\n",
    "    noise = np.random.randn(rows * cols, latent_dim)\n",
    "    imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    imgs = 0.5 * imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols)\n",
    "    idx = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            axs[i,j].imshow(imgs[idx].reshape(H, W), cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            idx += 1\n",
    "    fig.savefig(\"gan_images/%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d8871d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/30000, d_loss: 0.81, d_acc: 0.27, g_loss: 0.47\n",
      "epoch: 101/30000, d_loss: 0.03, d_acc: 1.00, g_loss: 3.74\n",
      "epoch: 201/30000, d_loss: 1.10, d_acc: 0.28, g_loss: 0.50\n",
      "epoch: 301/30000, d_loss: 0.69, d_acc: 0.45, g_loss: 0.63\n",
      "epoch: 401/30000, d_loss: 0.68, d_acc: 0.47, g_loss: 0.62\n",
      "epoch: 501/30000, d_loss: 0.68, d_acc: 0.50, g_loss: 0.66\n",
      "epoch: 601/30000, d_loss: 0.69, d_acc: 0.45, g_loss: 0.67\n",
      "epoch: 701/30000, d_loss: 0.66, d_acc: 0.48, g_loss: 0.68\n",
      "epoch: 801/30000, d_loss: 0.68, d_acc: 0.45, g_loss: 0.69\n",
      "epoch: 901/30000, d_loss: 0.67, d_acc: 0.59, g_loss: 0.71\n",
      "epoch: 1001/30000, d_loss: 0.70, d_acc: 0.47, g_loss: 0.71\n",
      "epoch: 1101/30000, d_loss: 0.69, d_acc: 0.50, g_loss: 0.73\n",
      "epoch: 1201/30000, d_loss: 0.70, d_acc: 0.42, g_loss: 0.77\n",
      "epoch: 1301/30000, d_loss: 0.66, d_acc: 0.56, g_loss: 0.80\n",
      "epoch: 1401/30000, d_loss: 0.68, d_acc: 0.53, g_loss: 0.76\n",
      "epoch: 1501/30000, d_loss: 0.59, d_acc: 0.75, g_loss: 0.81\n",
      "epoch: 1601/30000, d_loss: 0.66, d_acc: 0.56, g_loss: 0.77\n",
      "epoch: 1701/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.83\n",
      "epoch: 1801/30000, d_loss: 0.70, d_acc: 0.53, g_loss: 0.80\n",
      "epoch: 1901/30000, d_loss: 0.64, d_acc: 0.62, g_loss: 0.83\n",
      "epoch: 2001/30000, d_loss: 0.68, d_acc: 0.59, g_loss: 0.76\n",
      "epoch: 2101/30000, d_loss: 0.72, d_acc: 0.45, g_loss: 0.79\n",
      "epoch: 2201/30000, d_loss: 0.67, d_acc: 0.62, g_loss: 0.80\n",
      "epoch: 2301/30000, d_loss: 0.68, d_acc: 0.64, g_loss: 0.77\n",
      "epoch: 2401/30000, d_loss: 0.69, d_acc: 0.61, g_loss: 0.75\n",
      "epoch: 2501/30000, d_loss: 0.69, d_acc: 0.52, g_loss: 0.79\n",
      "epoch: 2601/30000, d_loss: 0.69, d_acc: 0.52, g_loss: 0.82\n",
      "epoch: 2701/30000, d_loss: 0.65, d_acc: 0.66, g_loss: 0.82\n",
      "epoch: 2801/30000, d_loss: 0.72, d_acc: 0.47, g_loss: 0.76\n",
      "epoch: 2901/30000, d_loss: 0.63, d_acc: 0.73, g_loss: 0.78\n",
      "epoch: 3001/30000, d_loss: 0.64, d_acc: 0.69, g_loss: 0.81\n",
      "epoch: 3101/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.74\n",
      "epoch: 3201/30000, d_loss: 0.65, d_acc: 0.64, g_loss: 0.75\n",
      "epoch: 3301/30000, d_loss: 0.67, d_acc: 0.56, g_loss: 0.77\n",
      "epoch: 3401/30000, d_loss: 0.66, d_acc: 0.61, g_loss: 0.75\n",
      "epoch: 3501/30000, d_loss: 0.65, d_acc: 0.61, g_loss: 0.79\n",
      "epoch: 3601/30000, d_loss: 0.71, d_acc: 0.48, g_loss: 0.82\n",
      "epoch: 3701/30000, d_loss: 0.66, d_acc: 0.61, g_loss: 0.74\n",
      "epoch: 3801/30000, d_loss: 0.69, d_acc: 0.58, g_loss: 0.78\n",
      "epoch: 3901/30000, d_loss: 0.71, d_acc: 0.44, g_loss: 0.84\n",
      "epoch: 4001/30000, d_loss: 0.68, d_acc: 0.50, g_loss: 0.80\n",
      "epoch: 4101/30000, d_loss: 0.66, d_acc: 0.64, g_loss: 0.77\n",
      "epoch: 4201/30000, d_loss: 0.68, d_acc: 0.55, g_loss: 0.74\n",
      "epoch: 4301/30000, d_loss: 0.66, d_acc: 0.64, g_loss: 0.79\n",
      "epoch: 4401/30000, d_loss: 0.67, d_acc: 0.62, g_loss: 0.79\n",
      "epoch: 4501/30000, d_loss: 0.69, d_acc: 0.52, g_loss: 0.79\n",
      "epoch: 4601/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.76\n",
      "epoch: 4701/30000, d_loss: 0.67, d_acc: 0.66, g_loss: 0.79\n",
      "epoch: 4801/30000, d_loss: 0.68, d_acc: 0.50, g_loss: 0.78\n",
      "epoch: 4901/30000, d_loss: 0.66, d_acc: 0.69, g_loss: 0.75\n",
      "epoch: 5001/30000, d_loss: 0.71, d_acc: 0.52, g_loss: 0.79\n",
      "epoch: 5101/30000, d_loss: 0.65, d_acc: 0.70, g_loss: 0.79\n",
      "epoch: 5201/30000, d_loss: 0.69, d_acc: 0.61, g_loss: 0.77\n",
      "epoch: 5301/30000, d_loss: 0.73, d_acc: 0.42, g_loss: 0.79\n",
      "epoch: 5401/30000, d_loss: 0.68, d_acc: 0.53, g_loss: 0.79\n",
      "epoch: 5501/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.80\n",
      "epoch: 5601/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.83\n",
      "epoch: 5701/30000, d_loss: 0.66, d_acc: 0.55, g_loss: 0.74\n",
      "epoch: 5801/30000, d_loss: 0.66, d_acc: 0.67, g_loss: 0.80\n",
      "epoch: 5901/30000, d_loss: 0.73, d_acc: 0.44, g_loss: 0.80\n",
      "epoch: 6001/30000, d_loss: 0.70, d_acc: 0.53, g_loss: 0.83\n",
      "epoch: 6101/30000, d_loss: 0.71, d_acc: 0.50, g_loss: 0.82\n",
      "epoch: 6201/30000, d_loss: 0.70, d_acc: 0.55, g_loss: 0.81\n",
      "epoch: 6301/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.76\n",
      "epoch: 6401/30000, d_loss: 0.70, d_acc: 0.48, g_loss: 0.79\n",
      "epoch: 6501/30000, d_loss: 0.65, d_acc: 0.62, g_loss: 0.88\n",
      "epoch: 6601/30000, d_loss: 0.74, d_acc: 0.41, g_loss: 0.79\n",
      "epoch: 6701/30000, d_loss: 0.68, d_acc: 0.53, g_loss: 0.81\n",
      "epoch: 6801/30000, d_loss: 0.69, d_acc: 0.50, g_loss: 0.81\n",
      "epoch: 6901/30000, d_loss: 0.66, d_acc: 0.59, g_loss: 0.82\n",
      "epoch: 7001/30000, d_loss: 0.65, d_acc: 0.62, g_loss: 0.77\n",
      "epoch: 7101/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.84\n",
      "epoch: 7201/30000, d_loss: 0.66, d_acc: 0.70, g_loss: 0.83\n",
      "epoch: 7301/30000, d_loss: 0.71, d_acc: 0.50, g_loss: 0.77\n",
      "epoch: 7401/30000, d_loss: 0.74, d_acc: 0.42, g_loss: 0.83\n",
      "epoch: 7501/30000, d_loss: 0.68, d_acc: 0.59, g_loss: 0.77\n",
      "epoch: 7601/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.83\n",
      "epoch: 7701/30000, d_loss: 0.65, d_acc: 0.66, g_loss: 0.73\n",
      "epoch: 7801/30000, d_loss: 0.73, d_acc: 0.47, g_loss: 0.84\n",
      "epoch: 7901/30000, d_loss: 0.68, d_acc: 0.52, g_loss: 0.84\n",
      "epoch: 8001/30000, d_loss: 0.72, d_acc: 0.48, g_loss: 0.84\n",
      "epoch: 8101/30000, d_loss: 0.72, d_acc: 0.50, g_loss: 0.74\n",
      "epoch: 8201/30000, d_loss: 0.66, d_acc: 0.62, g_loss: 0.74\n",
      "epoch: 8301/30000, d_loss: 0.70, d_acc: 0.61, g_loss: 0.79\n",
      "epoch: 8401/30000, d_loss: 0.72, d_acc: 0.45, g_loss: 0.83\n",
      "epoch: 8501/30000, d_loss: 0.68, d_acc: 0.61, g_loss: 0.78\n",
      "epoch: 8601/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.87\n",
      "epoch: 8701/30000, d_loss: 0.69, d_acc: 0.50, g_loss: 0.81\n",
      "epoch: 8801/30000, d_loss: 0.67, d_acc: 0.69, g_loss: 0.79\n",
      "epoch: 8901/30000, d_loss: 0.70, d_acc: 0.55, g_loss: 0.80\n",
      "epoch: 9001/30000, d_loss: 0.68, d_acc: 0.59, g_loss: 0.79\n",
      "epoch: 9101/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.83\n",
      "epoch: 9201/30000, d_loss: 0.73, d_acc: 0.45, g_loss: 0.80\n",
      "epoch: 9301/30000, d_loss: 0.65, d_acc: 0.64, g_loss: 0.84\n",
      "epoch: 9401/30000, d_loss: 0.72, d_acc: 0.47, g_loss: 0.78\n",
      "epoch: 9501/30000, d_loss: 0.71, d_acc: 0.53, g_loss: 0.79\n",
      "epoch: 9601/30000, d_loss: 0.67, d_acc: 0.59, g_loss: 0.78\n",
      "epoch: 9701/30000, d_loss: 0.73, d_acc: 0.50, g_loss: 0.78\n",
      "epoch: 9801/30000, d_loss: 0.66, d_acc: 0.58, g_loss: 0.83\n",
      "epoch: 9901/30000, d_loss: 0.64, d_acc: 0.69, g_loss: 0.80\n",
      "epoch: 10001/30000, d_loss: 0.69, d_acc: 0.58, g_loss: 0.83\n",
      "epoch: 10101/30000, d_loss: 0.67, d_acc: 0.64, g_loss: 0.76\n",
      "epoch: 10201/30000, d_loss: 0.64, d_acc: 0.69, g_loss: 0.76\n",
      "epoch: 10301/30000, d_loss: 0.70, d_acc: 0.55, g_loss: 0.81\n",
      "epoch: 10401/30000, d_loss: 0.67, d_acc: 0.56, g_loss: 0.79\n",
      "epoch: 10501/30000, d_loss: 0.72, d_acc: 0.52, g_loss: 0.74\n",
      "epoch: 10601/30000, d_loss: 0.68, d_acc: 0.50, g_loss: 0.81\n",
      "epoch: 10701/30000, d_loss: 0.65, d_acc: 0.59, g_loss: 0.79\n",
      "epoch: 10801/30000, d_loss: 0.67, d_acc: 0.55, g_loss: 0.82\n",
      "epoch: 10901/30000, d_loss: 0.68, d_acc: 0.55, g_loss: 0.78\n",
      "epoch: 11001/30000, d_loss: 0.64, d_acc: 0.70, g_loss: 0.82\n",
      "epoch: 11101/30000, d_loss: 0.71, d_acc: 0.50, g_loss: 0.81\n",
      "epoch: 11201/30000, d_loss: 0.68, d_acc: 0.52, g_loss: 0.78\n",
      "epoch: 11301/30000, d_loss: 0.69, d_acc: 0.58, g_loss: 0.77\n",
      "epoch: 11401/30000, d_loss: 0.66, d_acc: 0.62, g_loss: 0.81\n",
      "epoch: 11501/30000, d_loss: 0.75, d_acc: 0.39, g_loss: 0.82\n",
      "epoch: 11601/30000, d_loss: 0.69, d_acc: 0.48, g_loss: 0.75\n",
      "epoch: 11701/30000, d_loss: 0.70, d_acc: 0.50, g_loss: 0.81\n",
      "epoch: 11801/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.81\n",
      "epoch: 11901/30000, d_loss: 0.66, d_acc: 0.61, g_loss: 0.77\n",
      "epoch: 12001/30000, d_loss: 0.73, d_acc: 0.44, g_loss: 0.80\n",
      "epoch: 12101/30000, d_loss: 0.64, d_acc: 0.59, g_loss: 0.77\n",
      "epoch: 12201/30000, d_loss: 0.64, d_acc: 0.72, g_loss: 0.79\n",
      "epoch: 12301/30000, d_loss: 0.68, d_acc: 0.55, g_loss: 0.80\n",
      "epoch: 12401/30000, d_loss: 0.65, d_acc: 0.58, g_loss: 0.82\n",
      "epoch: 12501/30000, d_loss: 0.66, d_acc: 0.52, g_loss: 0.80\n",
      "epoch: 12601/30000, d_loss: 0.67, d_acc: 0.56, g_loss: 0.81\n",
      "epoch: 12701/30000, d_loss: 0.69, d_acc: 0.52, g_loss: 0.78\n",
      "epoch: 12801/30000, d_loss: 0.70, d_acc: 0.48, g_loss: 0.81\n",
      "epoch: 12901/30000, d_loss: 0.65, d_acc: 0.62, g_loss: 0.81\n",
      "epoch: 13001/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.84\n",
      "epoch: 13101/30000, d_loss: 0.75, d_acc: 0.47, g_loss: 0.78\n",
      "epoch: 13201/30000, d_loss: 0.68, d_acc: 0.59, g_loss: 0.78\n",
      "epoch: 13301/30000, d_loss: 0.64, d_acc: 0.69, g_loss: 0.84\n",
      "epoch: 13401/30000, d_loss: 0.73, d_acc: 0.58, g_loss: 0.74\n",
      "epoch: 13501/30000, d_loss: 0.71, d_acc: 0.55, g_loss: 0.84\n",
      "epoch: 13601/30000, d_loss: 0.69, d_acc: 0.64, g_loss: 0.78\n",
      "epoch: 13701/30000, d_loss: 0.69, d_acc: 0.47, g_loss: 0.75\n",
      "epoch: 13801/30000, d_loss: 0.65, d_acc: 0.58, g_loss: 0.77\n",
      "epoch: 13901/30000, d_loss: 0.70, d_acc: 0.50, g_loss: 0.80\n",
      "epoch: 14001/30000, d_loss: 0.68, d_acc: 0.58, g_loss: 0.77\n",
      "epoch: 14101/30000, d_loss: 0.69, d_acc: 0.62, g_loss: 0.76\n",
      "epoch: 14201/30000, d_loss: 0.70, d_acc: 0.47, g_loss: 0.86\n",
      "epoch: 14301/30000, d_loss: 0.64, d_acc: 0.59, g_loss: 0.78\n",
      "epoch: 14401/30000, d_loss: 0.73, d_acc: 0.45, g_loss: 0.81\n",
      "epoch: 14501/30000, d_loss: 0.66, d_acc: 0.62, g_loss: 0.86\n",
      "epoch: 14601/30000, d_loss: 0.66, d_acc: 0.59, g_loss: 0.79\n",
      "epoch: 14701/30000, d_loss: 0.67, d_acc: 0.56, g_loss: 0.80\n",
      "epoch: 14801/30000, d_loss: 0.66, d_acc: 0.61, g_loss: 0.77\n",
      "epoch: 14901/30000, d_loss: 0.67, d_acc: 0.62, g_loss: 0.79\n",
      "epoch: 15001/30000, d_loss: 0.73, d_acc: 0.42, g_loss: 0.75\n",
      "epoch: 15101/30000, d_loss: 0.64, d_acc: 0.67, g_loss: 0.77\n",
      "epoch: 15201/30000, d_loss: 0.65, d_acc: 0.61, g_loss: 0.87\n",
      "epoch: 15301/30000, d_loss: 0.65, d_acc: 0.62, g_loss: 0.80\n",
      "epoch: 15401/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.78\n",
      "epoch: 15501/30000, d_loss: 0.67, d_acc: 0.56, g_loss: 0.78\n",
      "epoch: 15601/30000, d_loss: 0.69, d_acc: 0.56, g_loss: 0.87\n",
      "epoch: 15701/30000, d_loss: 0.68, d_acc: 0.59, g_loss: 0.77\n",
      "epoch: 15801/30000, d_loss: 0.72, d_acc: 0.44, g_loss: 0.73\n",
      "epoch: 15901/30000, d_loss: 0.65, d_acc: 0.53, g_loss: 0.82\n",
      "epoch: 16001/30000, d_loss: 0.64, d_acc: 0.64, g_loss: 0.82\n",
      "epoch: 16101/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.78\n",
      "epoch: 16201/30000, d_loss: 0.69, d_acc: 0.45, g_loss: 0.85\n",
      "epoch: 16301/30000, d_loss: 0.67, d_acc: 0.55, g_loss: 0.80\n",
      "epoch: 16401/30000, d_loss: 0.70, d_acc: 0.56, g_loss: 0.82\n",
      "epoch: 16501/30000, d_loss: 0.69, d_acc: 0.58, g_loss: 0.82\n",
      "epoch: 16601/30000, d_loss: 0.67, d_acc: 0.55, g_loss: 0.80\n",
      "epoch: 16701/30000, d_loss: 0.72, d_acc: 0.53, g_loss: 0.84\n",
      "epoch: 16801/30000, d_loss: 0.67, d_acc: 0.47, g_loss: 0.84\n",
      "epoch: 16901/30000, d_loss: 0.66, d_acc: 0.52, g_loss: 0.82\n",
      "epoch: 17001/30000, d_loss: 0.66, d_acc: 0.58, g_loss: 0.80\n",
      "epoch: 17101/30000, d_loss: 0.75, d_acc: 0.41, g_loss: 0.81\n",
      "epoch: 17201/30000, d_loss: 0.69, d_acc: 0.52, g_loss: 0.77\n",
      "epoch: 17301/30000, d_loss: 0.66, d_acc: 0.59, g_loss: 0.88\n",
      "epoch: 17401/30000, d_loss: 0.67, d_acc: 0.58, g_loss: 0.80\n",
      "epoch: 17501/30000, d_loss: 0.67, d_acc: 0.58, g_loss: 0.83\n",
      "epoch: 17601/30000, d_loss: 0.66, d_acc: 0.56, g_loss: 0.84\n",
      "epoch: 17701/30000, d_loss: 0.71, d_acc: 0.53, g_loss: 0.82\n",
      "epoch: 17801/30000, d_loss: 0.67, d_acc: 0.64, g_loss: 0.84\n",
      "epoch: 17901/30000, d_loss: 0.68, d_acc: 0.55, g_loss: 0.80\n",
      "epoch: 18001/30000, d_loss: 0.63, d_acc: 0.70, g_loss: 0.85\n",
      "epoch: 18101/30000, d_loss: 0.72, d_acc: 0.44, g_loss: 0.79\n",
      "epoch: 18201/30000, d_loss: 0.65, d_acc: 0.61, g_loss: 0.81\n",
      "epoch: 18301/30000, d_loss: 0.66, d_acc: 0.55, g_loss: 0.79\n",
      "epoch: 18401/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.83\n",
      "epoch: 18501/30000, d_loss: 0.68, d_acc: 0.47, g_loss: 0.84\n",
      "epoch: 18601/30000, d_loss: 0.69, d_acc: 0.56, g_loss: 0.76\n",
      "epoch: 18701/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.84\n",
      "epoch: 18801/30000, d_loss: 0.66, d_acc: 0.61, g_loss: 0.81\n",
      "epoch: 18901/30000, d_loss: 0.69, d_acc: 0.58, g_loss: 0.80\n",
      "epoch: 19001/30000, d_loss: 0.75, d_acc: 0.36, g_loss: 0.81\n",
      "epoch: 19101/30000, d_loss: 0.66, d_acc: 0.61, g_loss: 0.82\n",
      "epoch: 19201/30000, d_loss: 0.72, d_acc: 0.47, g_loss: 0.80\n",
      "epoch: 19301/30000, d_loss: 0.62, d_acc: 0.62, g_loss: 0.81\n",
      "epoch: 19401/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.78\n",
      "epoch: 19501/30000, d_loss: 0.67, d_acc: 0.59, g_loss: 0.87\n",
      "epoch: 19601/30000, d_loss: 0.65, d_acc: 0.58, g_loss: 0.85\n",
      "epoch: 19701/30000, d_loss: 0.68, d_acc: 0.53, g_loss: 0.83\n",
      "epoch: 19801/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.80\n",
      "epoch: 19901/30000, d_loss: 0.68, d_acc: 0.48, g_loss: 0.81\n",
      "epoch: 20001/30000, d_loss: 0.66, d_acc: 0.67, g_loss: 0.81\n",
      "epoch: 20101/30000, d_loss: 0.64, d_acc: 0.67, g_loss: 0.84\n",
      "epoch: 20201/30000, d_loss: 0.68, d_acc: 0.55, g_loss: 0.74\n",
      "epoch: 20301/30000, d_loss: 0.72, d_acc: 0.47, g_loss: 0.83\n",
      "epoch: 20401/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.87\n",
      "epoch: 20501/30000, d_loss: 0.70, d_acc: 0.58, g_loss: 0.83\n",
      "epoch: 20601/30000, d_loss: 0.68, d_acc: 0.61, g_loss: 0.82\n",
      "epoch: 20701/30000, d_loss: 0.68, d_acc: 0.48, g_loss: 0.85\n",
      "epoch: 20801/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.81\n",
      "epoch: 20901/30000, d_loss: 0.68, d_acc: 0.55, g_loss: 0.88\n",
      "epoch: 21001/30000, d_loss: 0.73, d_acc: 0.53, g_loss: 0.80\n",
      "epoch: 21101/30000, d_loss: 0.72, d_acc: 0.55, g_loss: 0.77\n",
      "epoch: 21201/30000, d_loss: 0.69, d_acc: 0.64, g_loss: 0.81\n",
      "epoch: 21301/30000, d_loss: 0.65, d_acc: 0.64, g_loss: 0.88\n",
      "epoch: 21401/30000, d_loss: 0.70, d_acc: 0.52, g_loss: 0.85\n",
      "epoch: 21501/30000, d_loss: 0.72, d_acc: 0.48, g_loss: 0.75\n",
      "epoch: 21601/30000, d_loss: 0.64, d_acc: 0.62, g_loss: 0.78\n",
      "epoch: 21701/30000, d_loss: 0.70, d_acc: 0.55, g_loss: 0.82\n",
      "epoch: 21801/30000, d_loss: 0.71, d_acc: 0.47, g_loss: 0.84\n",
      "epoch: 21901/30000, d_loss: 0.69, d_acc: 0.61, g_loss: 0.84\n",
      "epoch: 22001/30000, d_loss: 0.65, d_acc: 0.70, g_loss: 0.82\n",
      "epoch: 22101/30000, d_loss: 0.67, d_acc: 0.59, g_loss: 0.89\n",
      "epoch: 22201/30000, d_loss: 0.69, d_acc: 0.56, g_loss: 0.85\n",
      "epoch: 22301/30000, d_loss: 0.69, d_acc: 0.47, g_loss: 0.82\n",
      "epoch: 22401/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.83\n",
      "epoch: 22501/30000, d_loss: 0.64, d_acc: 0.67, g_loss: 0.89\n",
      "epoch: 22601/30000, d_loss: 0.69, d_acc: 0.50, g_loss: 0.85\n",
      "epoch: 22701/30000, d_loss: 0.64, d_acc: 0.55, g_loss: 0.84\n",
      "epoch: 22801/30000, d_loss: 0.64, d_acc: 0.64, g_loss: 0.87\n",
      "epoch: 22901/30000, d_loss: 0.61, d_acc: 0.67, g_loss: 0.83\n",
      "epoch: 23001/30000, d_loss: 0.66, d_acc: 0.61, g_loss: 0.90\n",
      "epoch: 23101/30000, d_loss: 0.71, d_acc: 0.53, g_loss: 0.78\n",
      "epoch: 23201/30000, d_loss: 0.71, d_acc: 0.52, g_loss: 0.81\n",
      "epoch: 23301/30000, d_loss: 0.70, d_acc: 0.55, g_loss: 0.83\n",
      "epoch: 23401/30000, d_loss: 0.72, d_acc: 0.45, g_loss: 0.81\n",
      "epoch: 23501/30000, d_loss: 0.67, d_acc: 0.58, g_loss: 0.83\n",
      "epoch: 23601/30000, d_loss: 0.68, d_acc: 0.61, g_loss: 0.83\n",
      "epoch: 23701/30000, d_loss: 0.66, d_acc: 0.58, g_loss: 0.82\n",
      "epoch: 23801/30000, d_loss: 0.65, d_acc: 0.62, g_loss: 0.85\n",
      "epoch: 23901/30000, d_loss: 0.62, d_acc: 0.67, g_loss: 0.90\n",
      "epoch: 24001/30000, d_loss: 0.65, d_acc: 0.72, g_loss: 0.81\n",
      "epoch: 24101/30000, d_loss: 0.72, d_acc: 0.55, g_loss: 0.79\n",
      "epoch: 24201/30000, d_loss: 0.64, d_acc: 0.66, g_loss: 0.81\n",
      "epoch: 24301/30000, d_loss: 0.70, d_acc: 0.55, g_loss: 0.76\n",
      "epoch: 24401/30000, d_loss: 0.64, d_acc: 0.62, g_loss: 0.81\n",
      "epoch: 24501/30000, d_loss: 0.64, d_acc: 0.69, g_loss: 0.86\n",
      "epoch: 24601/30000, d_loss: 0.65, d_acc: 0.62, g_loss: 0.87\n",
      "epoch: 24701/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.79\n",
      "epoch: 24801/30000, d_loss: 0.70, d_acc: 0.55, g_loss: 0.89\n",
      "epoch: 24901/30000, d_loss: 0.70, d_acc: 0.41, g_loss: 0.83\n",
      "epoch: 25001/30000, d_loss: 0.71, d_acc: 0.53, g_loss: 0.88\n",
      "epoch: 25101/30000, d_loss: 0.66, d_acc: 0.58, g_loss: 0.87\n",
      "epoch: 25201/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.78\n",
      "epoch: 25301/30000, d_loss: 0.66, d_acc: 0.58, g_loss: 0.76\n",
      "epoch: 25401/30000, d_loss: 0.68, d_acc: 0.59, g_loss: 0.82\n",
      "epoch: 25501/30000, d_loss: 0.63, d_acc: 0.64, g_loss: 0.81\n",
      "epoch: 25601/30000, d_loss: 0.67, d_acc: 0.53, g_loss: 0.85\n",
      "epoch: 25701/30000, d_loss: 0.69, d_acc: 0.52, g_loss: 0.79\n",
      "epoch: 25801/30000, d_loss: 0.71, d_acc: 0.48, g_loss: 0.79\n",
      "epoch: 25901/30000, d_loss: 0.71, d_acc: 0.55, g_loss: 0.79\n",
      "epoch: 26001/30000, d_loss: 0.68, d_acc: 0.62, g_loss: 0.80\n",
      "epoch: 26101/30000, d_loss: 0.69, d_acc: 0.53, g_loss: 0.79\n",
      "epoch: 26201/30000, d_loss: 0.69, d_acc: 0.58, g_loss: 0.83\n",
      "epoch: 26301/30000, d_loss: 0.57, d_acc: 0.77, g_loss: 0.85\n",
      "epoch: 26401/30000, d_loss: 0.64, d_acc: 0.69, g_loss: 0.82\n",
      "epoch: 26501/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.80\n",
      "epoch: 26601/30000, d_loss: 0.70, d_acc: 0.50, g_loss: 0.81\n",
      "epoch: 26701/30000, d_loss: 0.65, d_acc: 0.64, g_loss: 0.85\n",
      "epoch: 26801/30000, d_loss: 0.67, d_acc: 0.56, g_loss: 0.86\n",
      "epoch: 26901/30000, d_loss: 0.67, d_acc: 0.52, g_loss: 0.79\n",
      "epoch: 27001/30000, d_loss: 0.68, d_acc: 0.52, g_loss: 0.80\n",
      "epoch: 27101/30000, d_loss: 0.74, d_acc: 0.42, g_loss: 0.80\n",
      "epoch: 27201/30000, d_loss: 0.71, d_acc: 0.56, g_loss: 0.82\n",
      "epoch: 27301/30000, d_loss: 0.71, d_acc: 0.50, g_loss: 0.83\n",
      "epoch: 27401/30000, d_loss: 0.72, d_acc: 0.55, g_loss: 0.83\n",
      "epoch: 27501/30000, d_loss: 0.69, d_acc: 0.47, g_loss: 0.84\n",
      "epoch: 27601/30000, d_loss: 0.64, d_acc: 0.59, g_loss: 0.80\n",
      "epoch: 27701/30000, d_loss: 0.64, d_acc: 0.58, g_loss: 0.80\n",
      "epoch: 27801/30000, d_loss: 0.66, d_acc: 0.62, g_loss: 0.84\n",
      "epoch: 27901/30000, d_loss: 0.67, d_acc: 0.59, g_loss: 0.80\n",
      "epoch: 28001/30000, d_loss: 0.64, d_acc: 0.61, g_loss: 0.89\n",
      "epoch: 28101/30000, d_loss: 0.67, d_acc: 0.61, g_loss: 0.87\n",
      "epoch: 28201/30000, d_loss: 0.66, d_acc: 0.62, g_loss: 0.83\n",
      "epoch: 28301/30000, d_loss: 0.67, d_acc: 0.55, g_loss: 0.79\n",
      "epoch: 28401/30000, d_loss: 0.68, d_acc: 0.56, g_loss: 0.81\n",
      "epoch: 28501/30000, d_loss: 0.67, d_acc: 0.48, g_loss: 0.81\n",
      "epoch: 28601/30000, d_loss: 0.69, d_acc: 0.55, g_loss: 0.82\n",
      "epoch: 28701/30000, d_loss: 0.72, d_acc: 0.45, g_loss: 0.86\n",
      "epoch: 28801/30000, d_loss: 0.65, d_acc: 0.55, g_loss: 0.84\n",
      "epoch: 28901/30000, d_loss: 0.68, d_acc: 0.53, g_loss: 0.80\n",
      "epoch: 29001/30000, d_loss: 0.62, d_acc: 0.64, g_loss: 0.84\n",
      "epoch: 29101/30000, d_loss: 0.71, d_acc: 0.55, g_loss: 0.78\n",
      "epoch: 29201/30000, d_loss: 0.68, d_acc: 0.62, g_loss: 0.81\n",
      "epoch: 29301/30000, d_loss: 0.65, d_acc: 0.67, g_loss: 0.81\n",
      "epoch: 29401/30000, d_loss: 0.62, d_acc: 0.67, g_loss: 0.79\n",
      "epoch: 29501/30000, d_loss: 0.70, d_acc: 0.53, g_loss: 0.81\n",
      "epoch: 29601/30000, d_loss: 0.67, d_acc: 0.66, g_loss: 0.77\n",
      "epoch: 29701/30000, d_loss: 0.71, d_acc: 0.44, g_loss: 0.81\n",
      "epoch: 29801/30000, d_loss: 0.65, d_acc: 0.66, g_loss: 0.82\n",
      "epoch: 29901/30000, d_loss: 0.62, d_acc: 0.67, g_loss: 0.83\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "for epoch in range(epochs):\n",
    "  ###########################\n",
    "  ### Train discriminator ###\n",
    "  ###########################\n",
    "  \n",
    "  # Select a random batch of images\n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "    real_imgs = x_train[idx]\n",
    "\n",
    "    # Generate fake images\n",
    "    noise = np.random.randn(batch_size, latent_dim)\n",
    "    fake_imgs = generator.predict(noise)\n",
    "\n",
    "    # Train the discriminator\n",
    "    # both loss and accuracy are returned\n",
    "    d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n",
    "    d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n",
    "    d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "    d_acc  = 0.5 * (d_acc_real + d_acc_fake)\n",
    "\n",
    "\n",
    "    #######################\n",
    "    ### Train generator ###\n",
    "    #######################\n",
    "\n",
    "    noise = np.random.randn(batch_size, latent_dim)\n",
    "    g_loss = combined_model_gen.train_on_batch(noise, ones)\n",
    "\n",
    "    # do it again!\n",
    "    noise = np.random.randn(batch_size, latent_dim)\n",
    "    g_loss = combined_model_gen.train_on_batch(noise, ones)\n",
    "\n",
    "    # Save the losses\n",
    "    d_losses.append(d_loss)\n",
    "    g_losses.append(g_loss)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch: {epoch+1}/{epochs}, d_loss: {d_loss:.2f}, d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}\")\n",
    "\n",
    "    if epoch % sample_period == 0:\n",
    "        sample_images(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584fb44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3340b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = generator.predict(noise[1].reshape(1, noise[0].shape[0])).reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27656f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = generator.predict(np.random.rand(1,100)).reshape(28,28)\n",
    "# plt.imshow(data, interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2e90047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.save('generator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7953d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forgery-detection",
   "language": "python",
   "name": "forgery-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
